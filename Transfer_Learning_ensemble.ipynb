{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs = np.load('./train_caches.npy')\n",
    "test_xs = np.load('./test_caches.npy')\n",
    "(x_train, train_ys), (x_test, test_ys) = cifar10.load_data()\n",
    "n_classes = 10 \n",
    "# reshape\n",
    "test_ys = test_ys.reshape(-1)\n",
    "train_ys = train_ys.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DataProvider(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self.n_sample = len(labels)\n",
    "        self.queue = list(range(self.n_sample))\n",
    "        random.shuffle(self.queue)\n",
    "\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.epoch_count = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if len(self.queue) < batch_size:\n",
    "            self.queue = list(range(self.n_sample))\n",
    "            self.epoch_count += 1\n",
    "        target_indices = self.queue[:batch_size]\n",
    "        del self.queue[:batch_size]\n",
    "        return self.images[target_indices], self.labels[target_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default() :\n",
    "    xs = tf.placeholder(dtype = tf.float32, shape = [None,2048])\n",
    "    ys = tf.placeholder(dtype = tf.int32 , shape=[None])\n",
    "    lr = tf.placeholder(dtype = tf.float32, shape = [])\n",
    "    \n",
    "    n_units=[1024, 1024]\n",
    "    layer = xs \n",
    "    for ind, units in enumerate(n_units):\n",
    "        with tf.variable_scope('layer_{}'.format(ind)):\n",
    "            layer = tf.layers.Dense(units, tf.nn.relu,\n",
    "                                tf.initializers.he_normal)(layer)\n",
    "    logits = tf.layers.Dense(n_classes)(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default() :\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(ys, logits)\n",
    "    loss = tf.identity(loss, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default() :\n",
    "    pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default() :\n",
    "    # metric\n",
    "    logits_cls = tf.argmax(logits, axis=1)\n",
    "    logits_cls = tf.cast(logits_cls, dtype=tf.int32)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(ys, logits_cls),tf.float32),\n",
    "                         name='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default() :    \n",
    "    train_op = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_session(graph_def):\n",
    "    with graph_def.as_default() :    \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "        sess = tf.Session()\n",
    "        init = tf.group(tf.global_variables_initializer(),\n",
    "                        tf.local_variables_initializer())\n",
    "        # saver \n",
    "        sess.run(init)\n",
    "        return sess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(sess, graph_def):\n",
    "    # hparam \n",
    "    batch_size = 100\n",
    "    min_loss = 1000000.0\n",
    "    learning_rate = 0.001\n",
    "    max_iter = 10000\n",
    "\n",
    "    train_loss_ch = []\n",
    "    train_acc_ch = []\n",
    "    val_loss_ch = []\n",
    "    val_acc_ch = []\n",
    "\n",
    "    dataprovider = DataProvider(images=train_xs, labels=train_ys)\n",
    "    \n",
    "    with graph_def.as_default() :\n",
    "        for i in range(max_iter):\n",
    "            # extract batch_xs, batch_ys\n",
    "            batch_xs, batch_ys = dataprovider.next_batch(batch_size)\n",
    "\n",
    "            # Training \n",
    "            sess.run(train_op, {xs: batch_xs, ys: batch_ys, lr: learning_rate})\n",
    "\n",
    "            # Validation \n",
    "            if i % 100 == 0 :\n",
    "                # Validate validation dataset \n",
    "                fetches=[loss, acc]\n",
    "                val_loss, val_acc = sess.run(fetches, \n",
    "                                             {xs: test_xs, ys: test_ys})\n",
    "                val_loss_ch.append(val_loss)\n",
    "                val_acc_ch.append(val_acc)\n",
    "\n",
    "                # Validate train dataset : extract randomly 10000 samples from train dataset \n",
    "                batch_xs, batch_ys = dataprovider.next_batch(10000)\n",
    "                train_loss, train_acc = sess.run(fetches,{xs: batch_xs, ys: batch_ys})\n",
    "                train_loss_ch.append(train_loss)\n",
    "                train_acc_ch.append(train_acc)\n",
    "\n",
    "                print('step : {} train loss : {:.4f} acc : {:.4f} | Val loss : {:.4f} acc : {:.4f}'.\\\n",
    "                format(i, train_loss, train_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Multi Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 train loss : 2.3230 acc : 0.0829 | Val loss : 2.3227 acc : 0.0809\n",
      "step : 100 train loss : 0.9641 acc : 0.8143 | Val loss : 0.9589 acc : 0.8128\n",
      "step : 200 train loss : 0.5820 acc : 0.8365 | Val loss : 0.5858 acc : 0.8340\n",
      "step : 300 train loss : 0.4721 acc : 0.8595 | Val loss : 0.4909 acc : 0.8504\n",
      "step : 400 train loss : 0.4502 acc : 0.8559 | Val loss : 0.4511 acc : 0.8550\n",
      "step : 500 train loss : 0.4093 acc : 0.8705 | Val loss : 0.4264 acc : 0.8620\n",
      "step : 600 train loss : 0.3930 acc : 0.8720 | Val loss : 0.4091 acc : 0.8662\n",
      "step : 700 train loss : 0.3854 acc : 0.8736 | Val loss : 0.3990 acc : 0.8683\n",
      "step : 800 train loss : 0.3578 acc : 0.8849 | Val loss : 0.3896 acc : 0.8700\n",
      "step : 900 train loss : 0.3703 acc : 0.8779 | Val loss : 0.3830 acc : 0.8721\n",
      "step : 1000 train loss : 0.3525 acc : 0.8845 | Val loss : 0.3778 acc : 0.8753\n",
      "step : 1100 train loss : 0.3459 acc : 0.8842 | Val loss : 0.3707 acc : 0.8753\n",
      "step : 1200 train loss : 0.3467 acc : 0.8860 | Val loss : 0.3670 acc : 0.8771\n",
      "step : 1300 train loss : 0.3222 acc : 0.8953 | Val loss : 0.3622 acc : 0.8781\n",
      "step : 1400 train loss : 0.3379 acc : 0.8870 | Val loss : 0.3597 acc : 0.8799\n",
      "step : 1500 train loss : 0.3251 acc : 0.8931 | Val loss : 0.3575 acc : 0.8805\n",
      "step : 1600 train loss : 0.3204 acc : 0.8922 | Val loss : 0.3528 acc : 0.8801\n",
      "step : 1700 train loss : 0.3241 acc : 0.8921 | Val loss : 0.3502 acc : 0.8833\n",
      "step : 1800 train loss : 0.3004 acc : 0.9020 | Val loss : 0.3472 acc : 0.8827\n",
      "step : 1900 train loss : 0.3171 acc : 0.8937 | Val loss : 0.3463 acc : 0.8836\n",
      "step : 2000 train loss : 0.3065 acc : 0.8984 | Val loss : 0.3453 acc : 0.8845\n",
      "step : 2100 train loss : 0.3026 acc : 0.8972 | Val loss : 0.3416 acc : 0.8845\n",
      "step : 2200 train loss : 0.3078 acc : 0.8980 | Val loss : 0.3392 acc : 0.8871\n",
      "step : 2300 train loss : 0.2844 acc : 0.9073 | Val loss : 0.3374 acc : 0.8856\n",
      "step : 2400 train loss : 0.3018 acc : 0.9000 | Val loss : 0.3371 acc : 0.8869\n",
      "step : 2500 train loss : 0.2921 acc : 0.9033 | Val loss : 0.3366 acc : 0.8874\n",
      "step : 2600 train loss : 0.2888 acc : 0.9019 | Val loss : 0.3337 acc : 0.8874\n",
      "step : 2700 train loss : 0.2949 acc : 0.9011 | Val loss : 0.3312 acc : 0.8890\n",
      "step : 2800 train loss : 0.2716 acc : 0.9107 | Val loss : 0.3301 acc : 0.8884\n",
      "step : 2900 train loss : 0.2892 acc : 0.9035 | Val loss : 0.3300 acc : 0.8896\n",
      "step : 3000 train loss : 0.2804 acc : 0.9075 | Val loss : 0.3302 acc : 0.8903\n",
      "step : 3100 train loss : 0.2773 acc : 0.9054 | Val loss : 0.3277 acc : 0.8891\n",
      "step : 3200 train loss : 0.2840 acc : 0.9048 | Val loss : 0.3251 acc : 0.8911\n",
      "step : 3300 train loss : 0.2608 acc : 0.9144 | Val loss : 0.3245 acc : 0.8891\n",
      "step : 3400 train loss : 0.2787 acc : 0.9056 | Val loss : 0.3246 acc : 0.8911\n",
      "step : 3500 train loss : 0.2706 acc : 0.9108 | Val loss : 0.3252 acc : 0.8933\n",
      "step : 3600 train loss : 0.2675 acc : 0.9104 | Val loss : 0.3231 acc : 0.8904\n",
      "step : 3700 train loss : 0.2746 acc : 0.9075 | Val loss : 0.3203 acc : 0.8912\n",
      "step : 3800 train loss : 0.2514 acc : 0.9153 | Val loss : 0.3200 acc : 0.8894\n",
      "step : 3900 train loss : 0.2693 acc : 0.9091 | Val loss : 0.3200 acc : 0.8916\n",
      "step : 4000 train loss : 0.2620 acc : 0.9141 | Val loss : 0.3212 acc : 0.8949\n",
      "step : 4100 train loss : 0.2588 acc : 0.9126 | Val loss : 0.3194 acc : 0.8917\n",
      "step : 4200 train loss : 0.2662 acc : 0.9110 | Val loss : 0.3164 acc : 0.8923\n",
      "step : 4300 train loss : 0.2432 acc : 0.9172 | Val loss : 0.3163 acc : 0.8909\n",
      "step : 4400 train loss : 0.2610 acc : 0.9111 | Val loss : 0.3162 acc : 0.8941\n",
      "step : 4500 train loss : 0.2543 acc : 0.9165 | Val loss : 0.3180 acc : 0.8962\n",
      "step : 4600 train loss : 0.2512 acc : 0.9161 | Val loss : 0.3164 acc : 0.8932\n",
      "step : 4700 train loss : 0.2585 acc : 0.9132 | Val loss : 0.3131 acc : 0.8933\n",
      "step : 4800 train loss : 0.2356 acc : 0.9200 | Val loss : 0.3133 acc : 0.8927\n",
      "step : 4900 train loss : 0.2534 acc : 0.9134 | Val loss : 0.3129 acc : 0.8956\n",
      "step : 5000 train loss : 0.2474 acc : 0.9193 | Val loss : 0.3152 acc : 0.8967\n",
      "step : 5100 train loss : 0.2439 acc : 0.9188 | Val loss : 0.3137 acc : 0.8946\n",
      "step : 5200 train loss : 0.2516 acc : 0.9160 | Val loss : 0.3105 acc : 0.8940\n",
      "step : 5300 train loss : 0.2287 acc : 0.9219 | Val loss : 0.3106 acc : 0.8939\n",
      "step : 5400 train loss : 0.2464 acc : 0.9152 | Val loss : 0.3102 acc : 0.8965\n",
      "step : 5500 train loss : 0.2409 acc : 0.9215 | Val loss : 0.3128 acc : 0.8973\n",
      "step : 5600 train loss : 0.2374 acc : 0.9208 | Val loss : 0.3116 acc : 0.8952\n",
      "step : 5700 train loss : 0.2451 acc : 0.9187 | Val loss : 0.3082 acc : 0.8959\n",
      "step : 5800 train loss : 0.2224 acc : 0.9243 | Val loss : 0.3083 acc : 0.8946\n",
      "step : 5900 train loss : 0.2398 acc : 0.9177 | Val loss : 0.3078 acc : 0.8969\n",
      "step : 6000 train loss : 0.2348 acc : 0.9232 | Val loss : 0.3108 acc : 0.8980\n",
      "step : 6100 train loss : 0.2312 acc : 0.9232 | Val loss : 0.3097 acc : 0.8955\n",
      "step : 6200 train loss : 0.2389 acc : 0.9201 | Val loss : 0.3063 acc : 0.8973\n",
      "step : 6300 train loss : 0.2164 acc : 0.9271 | Val loss : 0.3064 acc : 0.8954\n",
      "step : 6400 train loss : 0.2336 acc : 0.9209 | Val loss : 0.3056 acc : 0.8972\n",
      "step : 6500 train loss : 0.2291 acc : 0.9248 | Val loss : 0.3091 acc : 0.8980\n",
      "step : 6600 train loss : 0.2254 acc : 0.9247 | Val loss : 0.3081 acc : 0.8959\n",
      "step : 6700 train loss : 0.2331 acc : 0.9234 | Val loss : 0.3047 acc : 0.8983\n",
      "step : 6800 train loss : 0.2108 acc : 0.9292 | Val loss : 0.3046 acc : 0.8957\n",
      "step : 6900 train loss : 0.2277 acc : 0.9227 | Val loss : 0.3038 acc : 0.8985\n",
      "step : 7000 train loss : 0.2237 acc : 0.9271 | Val loss : 0.3076 acc : 0.8990\n",
      "step : 7100 train loss : 0.2198 acc : 0.9263 | Val loss : 0.3066 acc : 0.8972\n",
      "step : 7200 train loss : 0.2276 acc : 0.9254 | Val loss : 0.3033 acc : 0.8991\n",
      "step : 7300 train loss : 0.2055 acc : 0.9307 | Val loss : 0.3032 acc : 0.8962\n",
      "step : 7400 train loss : 0.2221 acc : 0.9244 | Val loss : 0.3022 acc : 0.8994\n",
      "step : 7500 train loss : 0.2185 acc : 0.9293 | Val loss : 0.3063 acc : 0.8997\n",
      "step : 7600 train loss : 0.2145 acc : 0.9287 | Val loss : 0.3054 acc : 0.8984\n",
      "step : 7700 train loss : 0.2223 acc : 0.9278 | Val loss : 0.3022 acc : 0.8998\n",
      "step : 7800 train loss : 0.2005 acc : 0.9320 | Val loss : 0.3019 acc : 0.8971\n",
      "step : 7900 train loss : 0.2168 acc : 0.9256 | Val loss : 0.3009 acc : 0.9002\n",
      "step : 8000 train loss : 0.2135 acc : 0.9304 | Val loss : 0.3052 acc : 0.8998\n",
      "step : 8100 train loss : 0.2094 acc : 0.9307 | Val loss : 0.3044 acc : 0.8993\n",
      "step : 8200 train loss : 0.2171 acc : 0.9299 | Val loss : 0.3012 acc : 0.8998\n",
      "step : 8300 train loss : 0.1956 acc : 0.9342 | Val loss : 0.3008 acc : 0.8978\n",
      "step : 8400 train loss : 0.2116 acc : 0.9288 | Val loss : 0.2998 acc : 0.9006\n",
      "step : 8500 train loss : 0.2087 acc : 0.9314 | Val loss : 0.3043 acc : 0.8997\n",
      "step : 8600 train loss : 0.2046 acc : 0.9323 | Val loss : 0.3036 acc : 0.9001\n",
      "step : 8700 train loss : 0.2122 acc : 0.9319 | Val loss : 0.3004 acc : 0.8997\n",
      "step : 8800 train loss : 0.1910 acc : 0.9359 | Val loss : 0.3000 acc : 0.8990\n",
      "step : 8900 train loss : 0.2066 acc : 0.9299 | Val loss : 0.2988 acc : 0.9009\n",
      "step : 9000 train loss : 0.2040 acc : 0.9335 | Val loss : 0.3035 acc : 0.9000\n",
      "step : 9100 train loss : 0.1999 acc : 0.9336 | Val loss : 0.3028 acc : 0.9004\n",
      "step : 9200 train loss : 0.2074 acc : 0.9334 | Val loss : 0.2997 acc : 0.8995\n",
      "step : 9300 train loss : 0.1865 acc : 0.9375 | Val loss : 0.2992 acc : 0.8999\n",
      "step : 9400 train loss : 0.2017 acc : 0.9316 | Val loss : 0.2979 acc : 0.9015\n",
      "step : 9500 train loss : 0.1996 acc : 0.9350 | Val loss : 0.3029 acc : 0.9007\n",
      "step : 9600 train loss : 0.1953 acc : 0.9359 | Val loss : 0.3022 acc : 0.9002\n",
      "step : 9700 train loss : 0.2027 acc : 0.9347 | Val loss : 0.2991 acc : 0.8995\n",
      "step : 9800 train loss : 0.1821 acc : 0.9386 | Val loss : 0.2985 acc : 0.9002\n",
      "step : 9900 train loss : 0.1969 acc : 0.9328 | Val loss : 0.2972 acc : 0.9013\n",
      "step : 0 train loss : 2.3435 acc : 0.0739 | Val loss : 2.3454 acc : 0.0705\n",
      "step : 100 train loss : 0.9503 acc : 0.8147 | Val loss : 0.9540 acc : 0.8126\n",
      "step : 200 train loss : 0.5742 acc : 0.8388 | Val loss : 0.5788 acc : 0.8385\n",
      "step : 300 train loss : 0.4738 acc : 0.8574 | Val loss : 0.4894 acc : 0.8509\n",
      "step : 400 train loss : 0.4446 acc : 0.8568 | Val loss : 0.4472 acc : 0.8581\n",
      "step : 500 train loss : 0.4051 acc : 0.8722 | Val loss : 0.4237 acc : 0.8636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 600 train loss : 0.3889 acc : 0.8729 | Val loss : 0.4060 acc : 0.8673\n",
      "step : 700 train loss : 0.3827 acc : 0.8730 | Val loss : 0.3957 acc : 0.8700\n",
      "step : 800 train loss : 0.3593 acc : 0.8828 | Val loss : 0.3884 acc : 0.8710\n",
      "step : 900 train loss : 0.3664 acc : 0.8751 | Val loss : 0.3793 acc : 0.8748\n",
      "step : 1000 train loss : 0.3496 acc : 0.8857 | Val loss : 0.3754 acc : 0.8757\n",
      "step : 1100 train loss : 0.3420 acc : 0.8857 | Val loss : 0.3681 acc : 0.8763\n",
      "step : 1200 train loss : 0.3441 acc : 0.8859 | Val loss : 0.3643 acc : 0.8808\n",
      "step : 1300 train loss : 0.3242 acc : 0.8925 | Val loss : 0.3617 acc : 0.8775\n",
      "step : 1400 train loss : 0.3358 acc : 0.8860 | Val loss : 0.3568 acc : 0.8809\n",
      "step : 1500 train loss : 0.3236 acc : 0.8916 | Val loss : 0.3565 acc : 0.8801\n",
      "step : 1600 train loss : 0.3174 acc : 0.8948 | Val loss : 0.3515 acc : 0.8809\n",
      "step : 1700 train loss : 0.3217 acc : 0.8938 | Val loss : 0.3486 acc : 0.8840\n",
      "step : 1800 train loss : 0.3027 acc : 0.8989 | Val loss : 0.3476 acc : 0.8826\n",
      "step : 1900 train loss : 0.3161 acc : 0.8940 | Val loss : 0.3441 acc : 0.8844\n",
      "step : 2000 train loss : 0.3061 acc : 0.8974 | Val loss : 0.3454 acc : 0.8833\n",
      "step : 2100 train loss : 0.3002 acc : 0.8989 | Val loss : 0.3412 acc : 0.8841\n",
      "step : 2200 train loss : 0.3058 acc : 0.8989 | Val loss : 0.3383 acc : 0.8876\n",
      "step : 2300 train loss : 0.2867 acc : 0.9041 | Val loss : 0.3383 acc : 0.8856\n",
      "step : 2400 train loss : 0.3013 acc : 0.8996 | Val loss : 0.3354 acc : 0.8880\n",
      "step : 2500 train loss : 0.2926 acc : 0.9031 | Val loss : 0.3377 acc : 0.8857\n",
      "step : 2600 train loss : 0.2869 acc : 0.9027 | Val loss : 0.3340 acc : 0.8871\n",
      "step : 2700 train loss : 0.2931 acc : 0.9022 | Val loss : 0.3310 acc : 0.8894\n",
      "step : 2800 train loss : 0.2738 acc : 0.9083 | Val loss : 0.3316 acc : 0.8885\n",
      "step : 2900 train loss : 0.2892 acc : 0.9036 | Val loss : 0.3288 acc : 0.8902\n",
      "step : 3000 train loss : 0.2816 acc : 0.9062 | Val loss : 0.3320 acc : 0.8878\n",
      "step : 3100 train loss : 0.2760 acc : 0.9058 | Val loss : 0.3287 acc : 0.8903\n",
      "step : 3200 train loss : 0.2823 acc : 0.9049 | Val loss : 0.3253 acc : 0.8912\n",
      "step : 3300 train loss : 0.2630 acc : 0.9105 | Val loss : 0.3263 acc : 0.8888\n",
      "step : 3400 train loss : 0.2790 acc : 0.9064 | Val loss : 0.3237 acc : 0.8913\n",
      "step : 3500 train loss : 0.2722 acc : 0.9089 | Val loss : 0.3275 acc : 0.8899\n",
      "step : 3600 train loss : 0.2666 acc : 0.9084 | Val loss : 0.3245 acc : 0.8917\n",
      "step : 3700 train loss : 0.2731 acc : 0.9075 | Val loss : 0.3207 acc : 0.8927\n",
      "step : 3800 train loss : 0.2535 acc : 0.9134 | Val loss : 0.3221 acc : 0.8902\n",
      "step : 3900 train loss : 0.2700 acc : 0.9093 | Val loss : 0.3195 acc : 0.8930\n",
      "step : 4000 train loss : 0.2638 acc : 0.9116 | Val loss : 0.3239 acc : 0.8918\n",
      "step : 4100 train loss : 0.2583 acc : 0.9112 | Val loss : 0.3211 acc : 0.8924\n",
      "step : 4200 train loss : 0.2648 acc : 0.9105 | Val loss : 0.3170 acc : 0.8946\n",
      "step : 4300 train loss : 0.2451 acc : 0.9165 | Val loss : 0.3187 acc : 0.8924\n",
      "step : 4400 train loss : 0.2618 acc : 0.9122 | Val loss : 0.3160 acc : 0.8936\n",
      "step : 4500 train loss : 0.2563 acc : 0.9141 | Val loss : 0.3210 acc : 0.8938\n",
      "step : 4600 train loss : 0.2508 acc : 0.9136 | Val loss : 0.3184 acc : 0.8939\n",
      "step : 4700 train loss : 0.2572 acc : 0.9125 | Val loss : 0.3139 acc : 0.8949\n",
      "step : 4800 train loss : 0.2376 acc : 0.9204 | Val loss : 0.3159 acc : 0.8943\n",
      "step : 4900 train loss : 0.2543 acc : 0.9147 | Val loss : 0.3130 acc : 0.8952\n",
      "step : 5000 train loss : 0.2494 acc : 0.9162 | Val loss : 0.3186 acc : 0.8947\n",
      "step : 5100 train loss : 0.2440 acc : 0.9160 | Val loss : 0.3161 acc : 0.8946\n",
      "step : 5200 train loss : 0.2503 acc : 0.9155 | Val loss : 0.3113 acc : 0.8969\n",
      "step : 5300 train loss : 0.2307 acc : 0.9224 | Val loss : 0.3135 acc : 0.8956\n",
      "step : 5400 train loss : 0.2474 acc : 0.9172 | Val loss : 0.3105 acc : 0.8968\n",
      "step : 5500 train loss : 0.2430 acc : 0.9188 | Val loss : 0.3165 acc : 0.8952\n",
      "step : 5600 train loss : 0.2376 acc : 0.9183 | Val loss : 0.3142 acc : 0.8964\n",
      "step : 5700 train loss : 0.2439 acc : 0.9183 | Val loss : 0.3092 acc : 0.8976\n",
      "step : 5800 train loss : 0.2243 acc : 0.9244 | Val loss : 0.3114 acc : 0.8966\n",
      "step : 5900 train loss : 0.2410 acc : 0.9187 | Val loss : 0.3083 acc : 0.8975\n",
      "step : 6000 train loss : 0.2369 acc : 0.9212 | Val loss : 0.3147 acc : 0.8959\n",
      "step : 6100 train loss : 0.2315 acc : 0.9217 | Val loss : 0.3124 acc : 0.8965\n",
      "step : 6200 train loss : 0.2378 acc : 0.9210 | Val loss : 0.3074 acc : 0.8980\n",
      "step : 6300 train loss : 0.2184 acc : 0.9259 | Val loss : 0.3097 acc : 0.8969\n",
      "step : 6400 train loss : 0.2349 acc : 0.9202 | Val loss : 0.3064 acc : 0.8983\n",
      "step : 6500 train loss : 0.2312 acc : 0.9231 | Val loss : 0.3132 acc : 0.8964\n",
      "step : 6600 train loss : 0.2258 acc : 0.9239 | Val loss : 0.3110 acc : 0.8970\n",
      "step : 6700 train loss : 0.2321 acc : 0.9219 | Val loss : 0.3058 acc : 0.8978\n",
      "step : 6800 train loss : 0.2128 acc : 0.9280 | Val loss : 0.3082 acc : 0.8981\n",
      "step : 6900 train loss : 0.2292 acc : 0.9231 | Val loss : 0.3048 acc : 0.8993\n",
      "step : 7000 train loss : 0.2257 acc : 0.9258 | Val loss : 0.3118 acc : 0.8969\n",
      "step : 7100 train loss : 0.2204 acc : 0.9257 | Val loss : 0.3098 acc : 0.8981\n",
      "step : 7200 train loss : 0.2266 acc : 0.9238 | Val loss : 0.3044 acc : 0.8986\n",
      "step : 7300 train loss : 0.2075 acc : 0.9293 | Val loss : 0.3069 acc : 0.8984\n",
      "step : 7400 train loss : 0.2236 acc : 0.9244 | Val loss : 0.3033 acc : 0.9001\n",
      "step : 7500 train loss : 0.2205 acc : 0.9279 | Val loss : 0.3106 acc : 0.8976\n",
      "step : 7600 train loss : 0.2152 acc : 0.9281 | Val loss : 0.3087 acc : 0.8983\n",
      "step : 7700 train loss : 0.2213 acc : 0.9252 | Val loss : 0.3032 acc : 0.8993\n",
      "step : 7800 train loss : 0.2025 acc : 0.9312 | Val loss : 0.3058 acc : 0.8986\n",
      "step : 7900 train loss : 0.2184 acc : 0.9262 | Val loss : 0.3021 acc : 0.9007\n",
      "step : 8000 train loss : 0.2154 acc : 0.9296 | Val loss : 0.3096 acc : 0.8981\n",
      "step : 8100 train loss : 0.2101 acc : 0.9312 | Val loss : 0.3077 acc : 0.8985\n",
      "step : 8200 train loss : 0.2162 acc : 0.9274 | Val loss : 0.3023 acc : 0.8998\n",
      "step : 8300 train loss : 0.1977 acc : 0.9338 | Val loss : 0.3049 acc : 0.8996\n",
      "step : 8400 train loss : 0.2133 acc : 0.9274 | Val loss : 0.3011 acc : 0.9005\n",
      "step : 8500 train loss : 0.2105 acc : 0.9312 | Val loss : 0.3087 acc : 0.8979\n",
      "step : 8600 train loss : 0.2052 acc : 0.9333 | Val loss : 0.3069 acc : 0.8993\n",
      "step : 8700 train loss : 0.2112 acc : 0.9298 | Val loss : 0.3015 acc : 0.9014\n",
      "step : 8800 train loss : 0.1931 acc : 0.9355 | Val loss : 0.3041 acc : 0.9000\n",
      "step : 8900 train loss : 0.2083 acc : 0.9290 | Val loss : 0.3001 acc : 0.9008\n",
      "step : 9000 train loss : 0.2058 acc : 0.9327 | Val loss : 0.3080 acc : 0.8983\n",
      "step : 9100 train loss : 0.2005 acc : 0.9345 | Val loss : 0.3062 acc : 0.8998\n",
      "step : 9200 train loss : 0.2064 acc : 0.9317 | Val loss : 0.3008 acc : 0.9010\n",
      "step : 9300 train loss : 0.1887 acc : 0.9370 | Val loss : 0.3035 acc : 0.8997\n",
      "step : 9400 train loss : 0.2035 acc : 0.9305 | Val loss : 0.2993 acc : 0.9016\n",
      "step : 9500 train loss : 0.2012 acc : 0.9338 | Val loss : 0.3074 acc : 0.8985\n",
      "step : 9600 train loss : 0.1959 acc : 0.9364 | Val loss : 0.3056 acc : 0.8991\n",
      "step : 9700 train loss : 0.2018 acc : 0.9338 | Val loss : 0.3003 acc : 0.9016\n",
      "step : 9800 train loss : 0.1843 acc : 0.9385 | Val loss : 0.3030 acc : 0.9004\n",
      "step : 9900 train loss : 0.1988 acc : 0.9322 | Val loss : 0.2987 acc : 0.9013\n"
     ]
    }
   ],
   "source": [
    "sessions = [] \n",
    "for i in range(2):\n",
    "    sess = open_session(graph)\n",
    "    training(sess, graph)\n",
    "    sessions.append(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.46635939e-04 1.98418205e-03 1.59415300e-04 ... 2.95155428e-06\n",
      "  1.13696195e-04 1.03503873e-04]\n",
      " [1.12957181e-02 9.77235287e-02 1.27472624e-04 ... 4.07008827e-07\n",
      "  8.90664101e-01 1.77655878e-04]\n",
      " [1.62587059e-03 8.04841635e-04 1.10394240e-06 ... 1.38604196e-06\n",
      "  9.97544289e-01 2.05346805e-05]\n",
      " ...\n",
      " [7.24042684e-06 8.29878536e-06 4.49760555e-05 ... 1.60747313e-05\n",
      "  5.87502700e-06 3.17081285e-05]\n",
      " [4.54786390e-01 3.91456664e-01 1.13720410e-01 ... 1.01676420e-03\n",
      "  1.03244651e-03 6.46431837e-03]\n",
      " [3.66316584e-04 9.60541438e-05 7.70299812e-04 ... 9.58935201e-01\n",
      "  6.16856323e-06 1.44658057e-04]]\n",
      "0.9015\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    pred1 = sessions[0].run(pred, {xs: test_xs})\n",
    "    pred2 = sessions[1].run(pred, {xs: test_xs})\n",
    "    \n",
    "    total_pred = (pred1 + pred2)/2.\n",
    "    print(total_pred)\n",
    "    \n",
    "total_cls = np.argmax(total_pred, axis=1)\n",
    "ensemble_acc = np.sum(np.equal(total_cls, test_ys)) / len(test_ys)\n",
    "print(ensemble_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
